**Architecture:** Encoder-Decoder Transformer\n**Shape:** The "Hourglass"\n**Concept:** Sequence-to-Sequence\n\nThe original Transformers were Encoder-Decoder models. They read text (Encoder), understand it, and then generate a new sequence (Decoder). Perfect for Translation and Summarization.\n\n### History\nIntroduced by Google in **2019**, T5 framed every NLP task as a "text-to-text" problem.
