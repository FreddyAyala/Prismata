# THE UNIVERSAL TRANSLATOR

**Architecture:** T5 (Encoder-Decoder)
**Shape:** The Hourglass
**Concept:** Sequence-to-Sequence

The original Transformers were Encoder-Decoder models. They read text (Encoder), understand it, and then generate a new sequence (Decoder).
T5 framed every NLP task as a "text-to-text" problem.

**Use Cases:**
*   **Translation:** "Translate English to German."
*   **Summarization:** condensing long articles into a single paragraph.
*   **Grammar Correction:** Rewriting messy sentences into professional prose.
